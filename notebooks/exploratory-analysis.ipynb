{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edef7e2f-01d4-4afe-90d0-6b7bb95593c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 2002 dataset saved to D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2002.csv\n",
      "Wave 2006 dataset saved to D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2006.csv\n",
      "Wave 2009 dataset saved to D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2009.csv\n",
      "Wave 2013 dataset saved to D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2013.csv\n",
      "Wave 2016 dataset saved to D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2016.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the dataset\n",
    "file_path = \"data/young_lives_ethiopia.csv\"\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# List of unique time categories\n",
    "time_categories = data['Time'].unique()\n",
    "\n",
    "# Save separate CSV files for each wave\n",
    "for time in time_categories:\n",
    "    # Filter the data for each time category\n",
    "    wave_data = data[data['Time'] == time]\n",
    "    \n",
    "    # Define the output file path for each wave\n",
    "    output_file = f\"D:\\\\Ph.D Course Material 2022\\\\My Dessertation\\\\0 My PhD Munuscript Drafts\\\\5 Machine Learning\\\\Dataset\\\\wave_{time}.csv\"\n",
    "    \n",
    "    # Save the wave data to a new CSV file\n",
    "    wave_data.to_csv(output_file, index=False)\n",
    "    print(f\"Wave {time} dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e78471-8415-40b0-8398-1f8f7f97823d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave_2002(2) dataset saved to D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2002(2).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the dataset\n",
    "file_path = \"data/young_lives_ethiopia.csv\"\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure \"Time\" and \"Subject\" columns exist\n",
    "if \"Time\" not in data.columns or \"Subject\" not in data.columns:\n",
    "    raise ValueError(\"The dataset must contain 'Time' and 'Subject' columns.\")\n",
    "\n",
    "# Columns to replace\n",
    "columns_to_replace = [\"Measles\", \"DPT\", \"Polio\"]\n",
    "\n",
    "# Create separate datasets for 2002 and 2006\n",
    "data_2002 = data[data[\"Time\"] == 2002].copy()\n",
    "data_2006 = data[data[\"Time\"] == 2006][[\"Subject\"] + columns_to_replace].copy()\n",
    "\n",
    "# Merge data_2002 with data_2006 based on \"Subject\" to update values\n",
    "wave_2002_2 = data_2002.merge(data_2006, on=\"Subject\", suffixes=(\"\", \"_2006\"), how=\"left\")\n",
    "\n",
    "# Replace 2002 values with 2006 values for selected columns\n",
    "for col in columns_to_replace:\n",
    "    wave_2002_2[col] = wave_2002_2[col + \"_2006\"]\n",
    "    wave_2002_2.drop(columns=[col + \"_2006\"], inplace=True)\n",
    "\n",
    "# Save the new dataset\n",
    "output_file = r\"D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\wave_2002(2).csv\"\n",
    "wave_2002_2.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Wave_2002(2) dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8470b051-d4d3-41fb-be72-49500396ccd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Subject  Time  Region  Residence  ch_sex  ch_age_mon        BMI  \\\n",
      "0        1  2002      14          1       1          14  15.111111   \n",
      "1        2  2002      14          1       1           8  16.219723   \n",
      "2        3  2002      14          1       1          12  12.602394   \n",
      "3        4  2002      14          1       1          11  18.845467   \n",
      "4        5  2002      14          1       2          13  14.167650   \n",
      "\n",
      "   ch_longterm_health_problem  ch_health_compared_peers  \\\n",
      "0                           0                         2   \n",
      "1                           0                         2   \n",
      "2                           0                         1   \n",
      "3                           0                         2   \n",
      "4                           0                         3   \n",
      "\n",
      "   ch_health_general_new  ...  Measles  DPT  Polio  HIB  delivery  chinjury  \\\n",
      "0                      2  ...      1.0  1.0    1.0    0         1         0   \n",
      "1                      1  ...      1.0  1.0    1.0    1         1         0   \n",
      "2                      3  ...      NaN  NaN    NaN    1         1         0   \n",
      "3                      3  ...      1.0  1.0    1.0    1         0         0   \n",
      "4                      2  ...      1.0  1.0    1.0    0         1         0   \n",
      "\n",
      "   dadlive  dadage  Head_Rela  Nutrition_Status  \n",
      "0        2      28          2                 1  \n",
      "1        1      32          1                 1  \n",
      "2        2      46          2                 8  \n",
      "3        1      40          1                 1  \n",
      "4        1      30          1                 1  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the dataset\n",
    "file_path = \"data/young_lives_ethiopia.csv\"\n",
    "\n",
    "# Import the dataset\n",
    "baseline_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(baseline_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e907366b-8654-4218-b4ad-c1c1e06f1b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequencies of Nutrition_Status at each time point:\n",
      "Nutrition_Status    1   2    3   4    5    6    8\n",
      "Time                                             \n",
      "2002              922  39  329  64  372  121  147\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Time' and 'Nutrition_Status', and count occurrences\n",
    "nutrition_frequencies = baseline_data.groupby(['Time', 'Nutrition_Status']).size().unstack(fill_value=0)\n",
    "\n",
    "# Display the frequencies\n",
    "print(\"\\nFrequencies of Nutrition_Status at each time point:\")\n",
    "print(nutrition_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b74cc4-7873-44dd-b0b3-f32893b34b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequencies of Nutrition_Status by gender at each time point:\n",
      "Nutrition_Status    1   2    3   4    5   6   8\n",
      "Time ch_sex                                    \n",
      "2002 1            433  17  195  38  208  65  91\n",
      "     2            489  22  134  26  164  56  56\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Time', 'ch_sex', and 'Nutrition_Status', and count occurrences\n",
    "nutrition_frequencies = baseline_data.groupby(['Time', 'ch_sex', 'Nutrition_Status']).size().unstack(fill_value=0)\n",
    "\n",
    "# Display the frequencies\n",
    "print(\"\\nFrequencies of Nutrition_Status by gender at each time point:\")\n",
    "print(nutrition_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b545fcca-4d4b-4234-a92c-16f88cf50a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the first three columns and select the rest as independent variables (X)\n",
    "X = baseline_data.iloc[:, 3:-1].values\n",
    "# target variable column name found the last column\n",
    "y = baseline_data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5ed88e-9f2a-49d0-8a1c-1dfbf370ac68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\getne\\AppData\\Local\\Temp\\ipykernel_12344\\351229467.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(X_df.applymap(lambda x: pd.to_numeric(x, errors='coerce')).isna().sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "5      0\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "9      0\n",
      "10     0\n",
      "11     0\n",
      "12     0\n",
      "13     0\n",
      "14     0\n",
      "15     0\n",
      "16     0\n",
      "17     0\n",
      "18     0\n",
      "19     0\n",
      "20     0\n",
      "21     0\n",
      "22     0\n",
      "23     0\n",
      "24     0\n",
      "25     0\n",
      "26     0\n",
      "27     0\n",
      "28     0\n",
      "29     0\n",
      "30    85\n",
      "31    85\n",
      "32    85\n",
      "33     0\n",
      "34     0\n",
      "35     0\n",
      "36     0\n",
      "37     0\n",
      "38     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the NumPy array to a DataFrame for inspection\n",
    "X_df = pd.DataFrame(X)\n",
    "\n",
    "# Check for non-numeric values and data types\n",
    "# print(X_df.head())\n",
    "print(X_df.applymap(lambda x: pd.to_numeric(x, errors='coerce')).isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e83a0c-8060-42b7-8bb0-47f4c9d23357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\getne\\AppData\\Local\\Temp\\ipykernel_12344\\2503071283.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X_cleaned_df.fillna(method='ffill', inplace=True)  # Forward fill to handle NaNs\n"
     ]
    }
   ],
   "source": [
    "# Convert non-numeric values to NaN and then handle missing values\n",
    "X_cleaned_df = X_df.apply(pd.to_numeric, errors='coerce')\n",
    "X_cleaned_df.fillna(method='ffill', inplace=True)  # Forward fill to handle NaNs\n",
    "\n",
    "# Convert back to a NumPy array\n",
    "X_cleaned = X_cleaned_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68193a92-4faf-4997-a80b-ace88a3409e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7450670-37d7-4d19-880b-be243ff7344d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scores:\n",
      "                          Feature       Score\n",
      "3                             BMI  313.349620\n",
      "21                Wealth_quintile   28.619384\n",
      "23                    toiletq_new   23.011521\n",
      "24          Access_to_electricity   20.742331\n",
      "0                       Residence   20.433547\n",
      "16                    MOM_Edu_New   13.391565\n",
      "13                    Dad_Edu_New   12.436565\n",
      "8                    Care_Edu_New   11.682959\n",
      "27                    ch_religion   11.558584\n",
      "2                      ch_age_mon   10.831148\n",
      "28           Num_antenatal_visits   10.429339\n",
      "17                   Head_Edu_New   10.324998\n",
      "22  Access_to_safe_drinking_water   10.231810\n",
      "6           ch_health_general_new   10.073030\n",
      "34                       delivery    9.453266\n",
      "25                   cookingq_new    7.890617\n",
      "31                            DPT    6.213124\n",
      "1                          ch_sex    4.522192\n",
      "4      ch_longterm_health_problem    3.954902\n",
      "30                        Measles    3.493670\n",
      "20                        hh_size    3.094410\n",
      "33                            HIB    2.979777\n",
      "29                            BCG    2.630431\n",
      "38                      Head_Rela    2.510997\n",
      "5        ch_health_compared_peers    2.434124\n",
      "15                        mom_age    2.413203\n",
      "35                       chinjury    2.213808\n",
      "37                         dadage    2.180317\n",
      "12                           CGSW    1.996563\n",
      "19                       head_sex    1.884740\n",
      "18                       head_age    1.648140\n",
      "9                   care_rln_head    1.605217\n",
      "36                        dadlive    1.501337\n",
      "26                   Ethnic_group    1.273387\n",
      "7                             CSW    1.217336\n",
      "10                       care_age    1.133912\n",
      "11                       care_sex    1.029392\n",
      "14              mom_live_location    0.732724\n",
      "32                          Polio    0.675266\n",
      "Feature scores saved to: D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\feature_scores.csv\n",
      "Selected features dataset saved to: D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\selected_features_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert to DataFrame if not already\n",
    "X_df = pd.DataFrame(X, columns=baseline_data.columns[3:-1])  # Use original column names\n",
    "\n",
    "# Ensure all values are numeric and handle missing data\n",
    "X_df = X_df.apply(pd.to_numeric, errors='coerce')\n",
    "X_df.fillna(X_df.mean(), inplace=True)  # Fill NaNs with column mean\n",
    "\n",
    "# Convert back to NumPy array\n",
    "X_cleaned = X_df.to_numpy()\n",
    "\n",
    "# Feature selection using SelectKBest\n",
    "k_best_features = 14  # Adjust as needed\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best_features)\n",
    "X_selected = selector.fit_transform(X_cleaned, y)  # Fit selector\n",
    "\n",
    "# Get selected feature names\n",
    "selected_feature_names = X_df.columns[selector.get_support()]  # Get names of selected features\n",
    "\n",
    "# Get the feature scores\n",
    "feature_scores_df = pd.DataFrame({\n",
    "    'Feature': X_df.columns,  # Use original feature names\n",
    "    'Score': selector.scores_\n",
    "})\n",
    "\n",
    "# Print the feature scores\n",
    "print(\"Feature scores:\")\n",
    "print(feature_scores_df.sort_values(by='Score', ascending=False))\n",
    "\n",
    "# Save feature scores to CSV\n",
    "feature_scores_save_path = r'D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\feature_scores.csv'\n",
    "feature_scores_df.to_csv(feature_scores_save_path, index=False)\n",
    "print(f\"Feature scores saved to: {feature_scores_save_path}\")\n",
    "\n",
    "# Standardize the selected features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "# Create a DataFrame with selected features\n",
    "selected_features_df = pd.DataFrame(X_scaled, columns=selected_feature_names)\n",
    "\n",
    "# Add the target variable to the DataFrame\n",
    "selected_features_df['Target'] = y\n",
    "\n",
    "# Save the dataset with selected features\n",
    "selected_features_save_path = r'D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\selected_features_dataset.csv'\n",
    "selected_features_df.to_csv(selected_features_save_path, index=False)\n",
    "\n",
    "print(f\"Selected features dataset saved to: {selected_features_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dede701-11a5-463c-ad7d-130776c0869a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baebac1-28cb-4746-a3af-c6add02a4a03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\getne\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\core.py:158: UserWarning: [11:05:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8838\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.76      0.71       187\n",
      "           2       0.98      0.98      0.98       168\n",
      "           3       0.85      0.81      0.83       198\n",
      "           4       0.97      0.99      0.98       178\n",
      "           5       0.81      0.73      0.77       185\n",
      "           6       0.98      0.97      0.98       193\n",
      "           8       0.97      0.95      0.96       182\n",
      "\n",
      "    accuracy                           0.88      1291\n",
      "   macro avg       0.89      0.89      0.89      1291\n",
      "weighted avg       0.89      0.88      0.88      1291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\baseline data.csv\")\n",
    "\n",
    "# Define independent (X) and target variable (y)\n",
    "X = df.iloc[:, 3:-1].values  # Excluding first 3 columns and last column as target\n",
    "y = df.iloc[:, -1].values    # Target variable is the last column\n",
    "\n",
    "# Step 1: Handle missing values (Imputation)\n",
    "imputer = SimpleImputer(strategy='mean')  # Using mean imputation\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Step 2: Encode class labels to start from 0\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Converts y to sequential numbers\n",
    "\n",
    "# Step 3: Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_imputed, y_encoded)\n",
    "\n",
    "# Step 4: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Step 5: Split data into train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train the XGBoost Classifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Step 8: Model Evaluation\n",
    "accuracy = accuracy_score(y_test_original, y_pred_original)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_original, y_pred_original))\n",
    "\n",
    "# # Step 9: Save the Model and Encoders\n",
    "# model_save_path = r\"D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\nutrition_model.pkl\"\n",
    "# label_encoder_save_path = r\"D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\label_encoder.pkl\"\n",
    "# scaler_save_path = r\"D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\scaler.pkl\"\n",
    "# imputer_save_path = r\"D:\\Ph.D Course Material 2022\\My Dessertation\\0 My PhD Munuscript Drafts\\5 Machine Learning\\Dataset\\imputer.pkl\"\n",
    "\n",
    "# joblib.dump(model, model_save_path)\n",
    "# joblib.dump(label_encoder, label_encoder_save_path)\n",
    "# joblib.dump(scaler, scaler_save_path)\n",
    "# joblib.dump(imputer, imputer_save_path)\n",
    "\n",
    "# print(f\"Model saved to: {model_save_path}\")\n",
    "# print(f\"Label encoder saved to: {label_encoder_save_path}\")\n",
    "# print(f\"Scaler saved to: {scaler_save_path}\")\n",
    "# print(f\"Imputer saved to: {imputer_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e10d98-6a12-4dff-8bca-c615d2e8ada3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4df93-6d61-4edc-a304-3c134e859edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f273e60a-f767-4022-8f0c-92250b57a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data first (BEFORE applying SMOTE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Apply SMOTE only to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling (After SMOTE)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)  # Only transform test set (no fitting)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
